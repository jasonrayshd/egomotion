# egomotion

We, as team "**TheSSVL**" or "**EgoMotion-COMPASS**", took **2nd** place in both **Object State Change Classification** and **PNR temporal localization** tasks in Ego4d Challenge 2022  


Moreover, **our work on Egocentric video understanding** will be made publicly available by Nov 2022.  



TODO

- [ ] Post Techincal report on Arxiv  
- [ ] Release codes which we used in Ego4d Challenge 2022  
- [ ] Release codes of our latest work on egocentric video understading  

### Reference
[1] VideoMAE by Zhan, etc : [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602)  
[2] VideoMAE by Kaiming, etc : [Masked Autoencoders As Spatiotemporal Learners](https://arxiv.org/abs/2205.09113)  
[3] Vanilla MAE: [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377)  
[4] Ego4D: [Ego4D: Around the World in 3,000 Hours of Egocentric Video](https://arxiv.org/abs/2110.07058)  


### Contact
If you have any questions about our projects or implementation, please open an issue or contact via email:  
Jiachen Lei: jiachenlei@zju.edu.cn 

### Acknowledgements
We built our codes based on [VideoMAE](https://github.com/MCG-NJU/VideoMAE), [MAE-pytorch](https://github.com/pengzhiliang/MAE-pytorch). Thanks to all the contributors of these great repositories.
